{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXPJKp0YWZhT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten,Conv1D,MaxPooling1D\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD, Adam, RMSprop,Adagrad\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KXNkaC7kWZhY",
    "outputId": "83f7f649-8a45-4650-e90b-40aaabc67f33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2972, 20)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "data= np.loadtxt('grayscale.csv', delimiter=',', dtype='float32')\n",
    "label=np.loadtxt('encodedlabel.csv',delimiter=',',dtype='float32')\n",
    "print(data.shape)\n",
    "Accu=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7174
    },
    "colab_type": "code",
    "id": "6sv6x40MWZhb",
    "outputId": "6d66b63c-01a0-4c54-f1dc-5ce70abc18eb",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2377, 20) (2377, 3) (595, 20) (595, 3)\n",
      "fold KFold(n_splits=6, random_state=1, shuffle=False)\n",
      "train: [[ 4.  1.  8. ...  0.  0.  0.]\n",
      " [ 2.  8.  8. ...  0.  0.  0.]\n",
      " [ 0. 65.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 4.  1.  8. ...  0.  0.  0.]\n",
      " [ 2.  0. 72. ...  0.  0.  0.]\n",
      " [ 4.  1.  8. ...  0.  0.  0.]], test: [[128.   1.   0. ...   0.   0.   0.]\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " ...\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " [  2.   4.   8. ...   0.   0.   0.]]\n",
      "(2476, 5, 4) (2476, 3) (496, 5, 4) (496, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 5, 32)             288       \n",
      "_________________________________________________________________\n",
      "flatten_49 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 128)               20608     \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 21,283\n",
      "Trainable params: 21,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1980 samples, validate on 496 samples\n",
      "Epoch 1/10\n",
      "1980/1980 [==============================] - 5s 2ms/step - loss: 0.4777 - acc: 0.7990 - val_loss: 0.1829 - val_acc: 0.9597\n",
      "Epoch 2/10\n",
      "1980/1980 [==============================] - 0s 103us/step - loss: 0.1749 - acc: 0.9480 - val_loss: 0.1329 - val_acc: 0.9698\n",
      "Epoch 3/10\n",
      "1980/1980 [==============================] - 0s 104us/step - loss: 0.1306 - acc: 0.9616 - val_loss: 0.1243 - val_acc: 0.9677\n",
      "Epoch 4/10\n",
      "1980/1980 [==============================] - 0s 100us/step - loss: 0.1252 - acc: 0.9672 - val_loss: 0.1192 - val_acc: 0.9657\n",
      "Epoch 5/10\n",
      "1980/1980 [==============================] - 0s 104us/step - loss: 0.1129 - acc: 0.9652 - val_loss: 0.1184 - val_acc: 0.9677\n",
      "Epoch 6/10\n",
      "1980/1980 [==============================] - 0s 100us/step - loss: 0.0986 - acc: 0.9712 - val_loss: 0.1146 - val_acc: 0.9698\n",
      "Epoch 7/10\n",
      "1980/1980 [==============================] - 0s 106us/step - loss: 0.0946 - acc: 0.9702 - val_loss: 0.1097 - val_acc: 0.9698\n",
      "Epoch 8/10\n",
      "1980/1980 [==============================] - 0s 102us/step - loss: 0.0895 - acc: 0.9768 - val_loss: 0.1236 - val_acc: 0.9718\n",
      "Epoch 9/10\n",
      "1980/1980 [==============================] - 0s 102us/step - loss: 0.1042 - acc: 0.9697 - val_loss: 0.1154 - val_acc: 0.9738\n",
      "Epoch 10/10\n",
      "1980/1980 [==============================] - 0s 107us/step - loss: 0.0824 - acc: 0.9778 - val_loss: 0.1156 - val_acc: 0.9657\n",
      "496/496 [==============================] - 0s 58us/step\n",
      "Accuracy: 97.58064516129032\n",
      "Confusion Matrix\n",
      "[[162   0   2]\n",
      " [  2 165   0]\n",
      " [  0   8 157]]\n",
      "Precision(Normal) 0.9878048780487805\n",
      "Precision(DOS) 0.953757225433526\n",
      "Precision(Probe) 0.9874213836477987\n",
      "Recall(Normal) 0.9878048780487805\n",
      "Recall(DOS) 0.9880239520958084\n",
      "Recall(Probe) 0.9515151515151515\n",
      "FScore(Normal) 0.9878048780487805\n",
      "FScore(DOS) 0.9705882352941175\n",
      "FScore(Probe) 0.9691358024691358\n",
      "kappa_score 0.9637059892435089\n",
      "fold KFold(n_splits=6, random_state=1, shuffle=False)\n",
      "train: [[128.   1.   0. ...   0.   0.   0.]\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " ...\n",
      " [  4.   1.   8. ...   0.   0.   0.]\n",
      " [  2.   0.  72. ...   0.   0.   0.]\n",
      " [  4.   1.   8. ...   0.   0.   0.]], test: [[  4.   1.   8. ...   0.   0.   0.]\n",
      " [  2.   8.   8. ...   0.   0.   0.]\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " ...\n",
      " [  4.   1.   8. ...   0.   0.   0.]\n",
      " [  2.   2.   8. ...   0.   0.   0.]\n",
      " [  0. 129.   0. ...   0.   0.   0.]]\n",
      "(2476, 5, 4) (2476, 3) (496, 5, 4) (496, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_50 (Conv1D)           (None, 5, 32)             288       \n",
      "_________________________________________________________________\n",
      "flatten_50 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 128)               20608     \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 21,283\n",
      "Trainable params: 21,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1980 samples, validate on 496 samples\n",
      "Epoch 1/10\n",
      "1980/1980 [==============================] - 4s 2ms/step - loss: 0.4703 - acc: 0.8167 - val_loss: 0.2374 - val_acc: 0.9395\n",
      "Epoch 2/10\n",
      "1980/1980 [==============================] - 0s 104us/step - loss: 0.2262 - acc: 0.9131 - val_loss: 0.1755 - val_acc: 0.9476\n",
      "Epoch 3/10\n",
      "1980/1980 [==============================] - 0s 101us/step - loss: 0.1889 - acc: 0.9318 - val_loss: 0.1724 - val_acc: 0.9516\n",
      "Epoch 4/10\n",
      "1980/1980 [==============================] - 0s 106us/step - loss: 0.1625 - acc: 0.9475 - val_loss: 0.1387 - val_acc: 0.9637\n",
      "Epoch 5/10\n",
      "1980/1980 [==============================] - 0s 103us/step - loss: 0.1419 - acc: 0.9576 - val_loss: 0.1312 - val_acc: 0.9657\n",
      "Epoch 6/10\n",
      "1980/1980 [==============================] - 0s 104us/step - loss: 0.1180 - acc: 0.9646 - val_loss: 0.1167 - val_acc: 0.9738\n",
      "Epoch 7/10\n",
      "1980/1980 [==============================] - 0s 104us/step - loss: 0.1044 - acc: 0.9747 - val_loss: 0.1109 - val_acc: 0.9758\n",
      "Epoch 8/10\n",
      "1980/1980 [==============================] - 0s 102us/step - loss: 0.1025 - acc: 0.9727 - val_loss: 0.1057 - val_acc: 0.9738\n",
      "Epoch 9/10\n",
      "1980/1980 [==============================] - 0s 105us/step - loss: 0.0977 - acc: 0.9722 - val_loss: 0.1163 - val_acc: 0.9758\n",
      "Epoch 10/10\n",
      "1980/1980 [==============================] - 0s 103us/step - loss: 0.1086 - acc: 0.9712 - val_loss: 0.1458 - val_acc: 0.9718\n",
      "496/496 [==============================] - 0s 66us/step\n",
      "Accuracy: 97.98387096774194\n",
      "Confusion Matrix\n",
      "[[157   1   1]\n",
      " [  0 176   0]\n",
      " [  1   7 153]]\n",
      "Precision(Normal) 0.9936708860759493\n",
      "Precision(DOS) 0.9565217391304348\n",
      "Precision(Probe) 0.9935064935064936\n",
      "Recall(Normal) 0.9874213836477987\n",
      "Recall(DOS) 1.0\n",
      "Recall(Probe) 0.9503105590062112\n",
      "FScore(Normal) 0.9905362776025236\n",
      "FScore(DOS) 0.9777777777777777\n",
      "FScore(Probe) 0.9714285714285714\n",
      "kappa_score 0.9697036331207701\n",
      "fold KFold(n_splits=6, random_state=1, shuffle=False)\n",
      "train: [[128.   1.   0. ...   0.   0.   0.]\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " ...\n",
      " [  4.   1.   8. ...   0.   0.   0.]\n",
      " [  2.   0.  72. ...   0.   0.   0.]\n",
      " [  4.   1.   8. ...   0.   0.   0.]], test: [[ 0. 65.  0. ...  0.  0.  0.]\n",
      " [ 4.  1.  8. ...  0.  0.  0.]\n",
      " [ 0. 65.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0. 65.  0. ...  0.  0.  0.]\n",
      " [ 0. 65.  0. ...  0.  0.  0.]\n",
      " [ 0. 65.  8. ...  0.  0.  0.]]\n",
      "(2477, 5, 4) (2477, 3) (495, 5, 4) (495, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 5, 32)             288       \n",
      "_________________________________________________________________\n",
      "flatten_51 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 128)               20608     \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 21,283\n",
      "Trainable params: 21,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1981 samples, validate on 496 samples\n",
      "Epoch 1/10\n",
      "1981/1981 [==============================] - 5s 2ms/step - loss: 0.5250 - acc: 0.8021 - val_loss: 0.2368 - val_acc: 0.9597\n",
      "Epoch 2/10\n",
      "1981/1981 [==============================] - 0s 108us/step - loss: 0.1755 - acc: 0.9591 - val_loss: 0.1489 - val_acc: 0.9617\n",
      "Epoch 3/10\n",
      "1981/1981 [==============================] - 0s 103us/step - loss: 0.1222 - acc: 0.9682 - val_loss: 0.1296 - val_acc: 0.9677\n",
      "Epoch 4/10\n",
      "1981/1981 [==============================] - 0s 106us/step - loss: 0.1129 - acc: 0.9717 - val_loss: 0.1200 - val_acc: 0.9698\n",
      "Epoch 5/10\n",
      "1981/1981 [==============================] - 0s 104us/step - loss: 0.1044 - acc: 0.9717 - val_loss: 0.1140 - val_acc: 0.9718\n",
      "Epoch 6/10\n",
      "1981/1981 [==============================] - 0s 101us/step - loss: 0.0966 - acc: 0.9743 - val_loss: 0.1172 - val_acc: 0.9758\n",
      "Epoch 7/10\n",
      "1981/1981 [==============================] - 0s 109us/step - loss: 0.0976 - acc: 0.9717 - val_loss: 0.1231 - val_acc: 0.9738\n",
      "Epoch 8/10\n",
      "1981/1981 [==============================] - 0s 106us/step - loss: 0.0913 - acc: 0.9748 - val_loss: 0.1109 - val_acc: 0.9778\n",
      "Epoch 9/10\n",
      "1981/1981 [==============================] - 0s 105us/step - loss: 0.0930 - acc: 0.9717 - val_loss: 0.1128 - val_acc: 0.9778\n",
      "Epoch 10/10\n",
      "1981/1981 [==============================] - 0s 104us/step - loss: 0.0901 - acc: 0.9783 - val_loss: 0.1184 - val_acc: 0.9758\n",
      "495/495 [==============================] - 0s 61us/step\n",
      "Accuracy: 97.17171718375852\n",
      "Confusion Matrix\n",
      "[[172   3   1]\n",
      " [  2 147   0]\n",
      " [  1   7 162]]\n",
      "Precision(Normal) 0.9828571428571429\n",
      "Precision(DOS) 0.9363057324840764\n",
      "Precision(Probe) 0.9938650306748467\n",
      "Recall(Normal) 0.9772727272727273\n",
      "Recall(DOS) 0.9865771812080537\n",
      "Recall(Probe) 0.9529411764705882\n",
      "FScore(Normal) 0.98005698005698\n",
      "FScore(DOS) 0.9607843137254902\n",
      "FScore(Probe) 0.972972972972973\n",
      "kappa_score 0.957516460072829\n",
      "fold KFold(n_splits=6, random_state=1, shuffle=False)\n",
      "train: [[128.   1.   0. ...   0.   0.   0.]\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " ...\n",
      " [  4.   1.   8. ...   0.   0.   0.]\n",
      " [  2.   0.  72. ...   0.   0.   0.]\n",
      " [  4.   1.   8. ...   0.   0.   0.]], test: [[  0.  65.   0. ...   0.   0.   0.]\n",
      " [  4.   1.   8. ...   0.   0.   0.]\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " ...\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " [128.   1.   0. ...   0.   0.   0.]\n",
      " [  4.   1.   8. ...   0.   0.   0.]]\n",
      "(2477, 5, 4) (2477, 3) (495, 5, 4) (495, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_52 (Conv1D)           (None, 5, 32)             288       \n",
      "_________________________________________________________________\n",
      "flatten_52 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 128)               20608     \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 21,283\n",
      "Trainable params: 21,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1981 samples, validate on 496 samples\n",
      "Epoch 1/10\n",
      "1981/1981 [==============================] - 5s 2ms/step - loss: 0.3977 - acc: 0.8647 - val_loss: 0.1831 - val_acc: 0.9698\n",
      "Epoch 2/10\n",
      "1981/1981 [==============================] - 0s 108us/step - loss: 0.1453 - acc: 0.9652 - val_loss: 0.1301 - val_acc: 0.9677\n",
      "Epoch 3/10\n",
      "1981/1981 [==============================] - 0s 103us/step - loss: 0.1126 - acc: 0.9687 - val_loss: 0.1226 - val_acc: 0.9677\n",
      "Epoch 4/10\n",
      "1981/1981 [==============================] - 0s 98us/step - loss: 0.1100 - acc: 0.9677 - val_loss: 0.1148 - val_acc: 0.9698\n",
      "Epoch 5/10\n",
      "1981/1981 [==============================] - 0s 104us/step - loss: 0.1127 - acc: 0.9702 - val_loss: 0.1148 - val_acc: 0.9698\n",
      "Epoch 6/10\n",
      "1981/1981 [==============================] - 0s 104us/step - loss: 0.0985 - acc: 0.9743 - val_loss: 0.1103 - val_acc: 0.9698\n",
      "Epoch 7/10\n",
      "1981/1981 [==============================] - 0s 107us/step - loss: 0.1022 - acc: 0.9707 - val_loss: 0.1023 - val_acc: 0.9718\n",
      "Epoch 8/10\n",
      "1981/1981 [==============================] - 0s 107us/step - loss: 0.0932 - acc: 0.9753 - val_loss: 0.1026 - val_acc: 0.9718\n",
      "Epoch 9/10\n",
      "1981/1981 [==============================] - 0s 124us/step - loss: 0.0882 - acc: 0.9717 - val_loss: 0.1027 - val_acc: 0.9718\n",
      "Epoch 10/10\n",
      "1981/1981 [==============================] - 0s 130us/step - loss: 0.0904 - acc: 0.9743 - val_loss: 0.1073 - val_acc: 0.9738\n",
      "495/495 [==============================] - 0s 70us/step\n",
      "Accuracy: 97.77777777777777\n",
      "Confusion Matrix\n",
      "[[173   0   1]\n",
      " [  0 151   0]\n",
      " [  2   8 160]]\n",
      "Precision(Normal) 0.9885714285714285\n",
      "Precision(DOS) 0.949685534591195\n",
      "Precision(Probe) 0.9937888198757764\n",
      "Recall(Normal) 0.9942528735632183\n",
      "Recall(DOS) 1.0\n",
      "Recall(Probe) 0.9411764705882353\n",
      "FScore(Normal) 0.991404011461318\n",
      "FScore(DOS) 0.9741935483870968\n",
      "FScore(Probe) 0.9667673716012085\n",
      "kappa_score 0.9666352116473443\n",
      "fold KFold(n_splits=6, random_state=1, shuffle=False)\n",
      "train: [[128.   1.   0. ...   0.   0.   0.]\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " ...\n",
      " [  4.   1.   8. ...   0.   0.   0.]\n",
      " [  2.   0.  72. ...   0.   0.   0.]\n",
      " [  4.   1.   8. ...   0.   0.   0.]], test: [[ 2.  8.  8. ...  0.  0.  0.]\n",
      " [ 0. 65.  0. ...  0.  0.  0.]\n",
      " [ 4.  1.  8. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0. 65.  0. ...  0.  0.  0.]\n",
      " [ 0. 65.  0. ...  0.  0.  0.]\n",
      " [ 0. 65.  8. ...  0.  0.  0.]]\n",
      "(2477, 5, 4) (2477, 3) (495, 5, 4) (495, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_53 (Conv1D)           (None, 5, 32)             288       \n",
      "_________________________________________________________________\n",
      "flatten_53 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 128)               20608     \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 21,283\n",
      "Trainable params: 21,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1981 samples, validate on 496 samples\n",
      "Epoch 1/10\n",
      "1981/1981 [==============================] - 5s 2ms/step - loss: 0.5334 - acc: 0.7976 - val_loss: 0.2554 - val_acc: 0.9153\n",
      "Epoch 2/10\n",
      "1981/1981 [==============================] - 0s 105us/step - loss: 0.2413 - acc: 0.9137 - val_loss: 0.1928 - val_acc: 0.9476\n",
      "Epoch 3/10\n",
      "1981/1981 [==============================] - 0s 105us/step - loss: 0.1715 - acc: 0.9409 - val_loss: 0.1548 - val_acc: 0.9597\n",
      "Epoch 4/10\n",
      "1981/1981 [==============================] - 0s 107us/step - loss: 0.1454 - acc: 0.9551 - val_loss: 0.1336 - val_acc: 0.9597\n",
      "Epoch 5/10\n",
      "1981/1981 [==============================] - 0s 104us/step - loss: 0.1204 - acc: 0.9687 - val_loss: 0.1264 - val_acc: 0.9738\n",
      "Epoch 6/10\n",
      "1981/1981 [==============================] - 0s 108us/step - loss: 0.1057 - acc: 0.9763 - val_loss: 0.1395 - val_acc: 0.9597\n",
      "Epoch 7/10\n",
      "1981/1981 [==============================] - 0s 107us/step - loss: 0.1053 - acc: 0.9753 - val_loss: 0.1540 - val_acc: 0.9798\n",
      "Epoch 8/10\n",
      "1981/1981 [==============================] - 0s 103us/step - loss: 0.0944 - acc: 0.9753 - val_loss: 0.1120 - val_acc: 0.9738\n",
      "Epoch 9/10\n",
      "1981/1981 [==============================] - 0s 105us/step - loss: 0.0942 - acc: 0.9753 - val_loss: 0.1630 - val_acc: 0.9577\n",
      "Epoch 10/10\n",
      "1981/1981 [==============================] - 0s 104us/step - loss: 0.0958 - acc: 0.9763 - val_loss: 0.0969 - val_acc: 0.9738\n",
      "495/495 [==============================] - 0s 63us/step\n",
      "Accuracy: 96.96969698173831\n",
      "Confusion Matrix\n",
      "[[149   0   3]\n",
      " [  1 161   1]\n",
      " [  3   7 170]]\n",
      "Precision(Normal) 0.9738562091503268\n",
      "Precision(DOS) 0.9583333333333334\n",
      "Precision(Probe) 0.9770114942528736\n",
      "Recall(Normal) 0.9802631578947368\n",
      "Recall(DOS) 0.9877300613496932\n",
      "Recall(Probe) 0.9444444444444444\n",
      "FScore(Normal) 0.9770491803278688\n",
      "FScore(DOS) 0.972809667673716\n",
      "FScore(Probe) 0.96045197740113\n",
      "kappa_score 0.9544660104866157\n",
      "fold KFold(n_splits=6, random_state=1, shuffle=False)\n",
      "train: [[128.   1.   0. ...   0.   0.   0.]\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " ...\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " [  0.  65.   0. ...   0.   0.   0.]\n",
      " [  0.  65.   8. ...   0.   0.   0.]], test: [[ 0. 65.  0. ...  0.  0.  0.]\n",
      " [ 0. 65.  0. ...  0.  0.  0.]\n",
      " [ 0. 65.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 4.  1.  8. ...  0.  0.  0.]\n",
      " [ 2.  0. 72. ...  0.  0.  0.]\n",
      " [ 4.  1.  8. ...  0.  0.  0.]]\n",
      "(2477, 5, 4) (2477, 3) (495, 5, 4) (495, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_54 (Conv1D)           (None, 5, 32)             288       \n",
      "_________________________________________________________________\n",
      "flatten_54 (Flatten)         (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 128)               20608     \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 21,283\n",
      "Trainable params: 21,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1981 samples, validate on 496 samples\n",
      "Epoch 1/10\n",
      "1981/1981 [==============================] - 5s 2ms/step - loss: 0.5018 - acc: 0.7996 - val_loss: 0.2214 - val_acc: 0.9415\n",
      "Epoch 2/10\n",
      "1981/1981 [==============================] - 0s 113us/step - loss: 0.1754 - acc: 0.9490 - val_loss: 0.1526 - val_acc: 0.9395\n",
      "Epoch 3/10\n",
      "1981/1981 [==============================] - 0s 107us/step - loss: 0.1230 - acc: 0.9662 - val_loss: 0.1111 - val_acc: 0.9738\n",
      "Epoch 4/10\n",
      "1981/1981 [==============================] - 0s 108us/step - loss: 0.1061 - acc: 0.9702 - val_loss: 0.1044 - val_acc: 0.9758\n",
      "Epoch 5/10\n",
      "1981/1981 [==============================] - 0s 104us/step - loss: 0.1060 - acc: 0.9682 - val_loss: 0.1410 - val_acc: 0.9738\n",
      "Epoch 6/10\n",
      "1981/1981 [==============================] - 0s 105us/step - loss: 0.0940 - acc: 0.9763 - val_loss: 0.0918 - val_acc: 0.9738\n",
      "Epoch 7/10\n",
      "1981/1981 [==============================] - 0s 111us/step - loss: 0.0918 - acc: 0.9758 - val_loss: 0.1034 - val_acc: 0.9758\n",
      "Epoch 8/10\n",
      "1981/1981 [==============================] - 0s 103us/step - loss: 0.0835 - acc: 0.9788 - val_loss: 0.0994 - val_acc: 0.9738\n",
      "Epoch 9/10\n",
      "1981/1981 [==============================] - 0s 105us/step - loss: 0.0856 - acc: 0.9788 - val_loss: 0.1044 - val_acc: 0.9758\n",
      "Epoch 10/10\n",
      "1981/1981 [==============================] - 0s 104us/step - loss: 0.0857 - acc: 0.9773 - val_loss: 0.1096 - val_acc: 0.9758\n",
      "495/495 [==============================] - 0s 64us/step\n",
      "Accuracy: 97.37373737373738\n",
      "Confusion Matrix\n",
      "[[144   1   0]\n",
      " [  4 179   1]\n",
      " [  0   7 159]]\n",
      "Precision(Normal) 0.972972972972973\n",
      "Precision(DOS) 0.9572192513368984\n",
      "Precision(Probe) 0.99375\n",
      "Recall(Normal) 0.993103448275862\n",
      "Recall(DOS) 0.9728260869565217\n",
      "Recall(Probe) 0.9578313253012049\n",
      "FScore(Normal) 0.9829351535836178\n",
      "FScore(DOS) 0.9649595687331537\n",
      "FScore(Probe) 0.9754601226993865\n",
      "kappa_score 0.9604236240521042\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, train_label, test_label = train_test_split(data, label, test_size=0.20, random_state=42)\n",
    "print(train_data.shape, train_label.shape,test_data.shape, test_label.shape)\n",
    "kfold = KFold(6, False, 1)\n",
    "for train, test in kfold.split(data):\n",
    "    \n",
    "    print('fold',kfold)\n",
    "    print('train: %s, test: %s' % (data[train], data[test]))\n",
    "    train_data= data[train].reshape(train.shape[0],5,4)\n",
    "    test_data= data[test].reshape(test.shape[0],5,4)\n",
    "    train_label=label[train]\n",
    "    test_label=label[test]\n",
    "    print(train_data.shape, train_label.shape,test_data.shape, test_label.shape)\n",
    "    \n",
    "    model= Sequential()\n",
    "    model.add(Conv1D(strides=1, input_shape=(5,4), padding=\"same\", activation=\"tanh\", filters=32,kernel_size=2)),\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128, input_dim=20, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    adam=Adam(lr=0.001)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    model.fit(train_data, train_label, epochs=10, batch_size=None,validation_split=0.2)\n",
    "\n",
    "    scores= model.evaluate(test_data,test_label)\n",
    "    print(\"Accuracy:\",scores[1]*100)\n",
    "    pred=model.predict(test_data)\n",
    "    cm=confusion_matrix(test_label.argmax(axis=1),pred.argmax(axis=1))\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    PN=(np.float(cm[0][0])/np.float(np.sum(cm[:,0])))\n",
    "    PD=(np.float(cm[1][1])/np.float(np.sum(cm[:,1])))\n",
    "    PP=(np.float(cm[2][2])/np.float(np.sum(cm[:,2])))\n",
    "\n",
    "    print('Precision(Normal)',PN)\n",
    "    print('Precision(DOS)',PD)\n",
    "    print('Precision(Probe)',PP)\n",
    "\n",
    "    RN=(np.float(cm[0][0])/np.float(np.sum(cm[0,:])))\n",
    "    RD=(np.float(cm[1][1])/np.float(np.sum(cm[1,:])))\n",
    "    RP=(np.float(cm[2][2])/np.float(np.sum(cm[2,:])))\n",
    "    print('Recall(Normal)',RN)\n",
    "    print('Recall(DOS)',RD)\n",
    "    print('Recall(Probe)',RP)\n",
    "    \n",
    "    FSN=2*(PN*RN)/(PN+RN)\n",
    "    FSD=2*(PD*RD)/(PD+RD)\n",
    "    FSP=2*(PP*RP)/(PP+RP)\n",
    "    print('FScore(Normal)',FSN)\n",
    "    print('FScore(DOS)',FSD)\n",
    "    print('FScore(Probe)',FSP)\n",
    "\n",
    "    kappa_score=cohen_kappa_score(test_label.argmax(axis=1),pred.argmax(axis=1))\n",
    "    print('kappa_score',kappa_score)\n",
    "    Accu.append(float(scores[1]))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "colab_type": "code",
    "id": "xwwOBTj2WZhe",
    "outputId": "6fc389c9-55a1-4887-f50d-c0dc7861fadd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9758064516129032, 0.9798387096774194, 0.9717171718375851, 0.9777777777777777, 0.9696969698173831, 0.9737373737373738]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEjCAYAAADDry0IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n25jw5LonlQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(Accu)\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Boxplot for 6 fold for CNN', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.boxplot(Accu)\n",
    "ax.set_xlabel('CNN')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "plt.show()\n",
    "matplotlib.rc(\"lines\", markeredgewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxMsiIwIZ7bH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CNN_KFold.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
